<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8" />
<title>Voice Eye Assistant</title>
<style>
  body { font-family: Arial, sans-serif; text-align:center; padding:40px; }
  button { padding:12px 20px; margin:8px; font-size:16px; }
  #status { margin-top:12px; font-weight:bold; }
</style>
</head>
<body>
  <h1 style="color: blue;font-size: large;">Eye Assistant</h1>
  <p>Kindly, click stop talking button after the discussion, other wise it will be continuousely capturing your voice</p>
  <button id="startBtn" style="color: green; padding: 5px;">Start talking</button>
  <button id="stopBtn" disabled style="color: green; padding: 5px;">Stop talking</button>
  <div id="status">Idle</div>

<script>
/* ---------------- CONFIG ---------------- */
const NGROK_WS = "wss://568eca15393f.ngrok-free.app/asr/ws";   // <- UPDATE THIS
const SAMPLE_RATE = 16000;

/* ------------- STATE VARIABLES ---------- */
let audioCtx;           // Playback AudioContext
let micCtx;             // Recording AudioContext
let mediaStream;
let processor;
let sourceNode;
let ws;
let running = false;
let playQueue = [];
let playing = false;

const statusEl = document.getElementById('status');
const startBtn = document.getElementById('startBtn');
const stopBtn = document.getElementById('stopBtn');

/* ------------ UI Helpers ---------------- */
function setStatus(s, color='black') {
  statusEl.textContent = s;
  statusEl.style.color = color;
}

/* ------------ Playback Helpers ---------- */
async function playWavArrayBuffer(ab) {
  try {
    const decoded = await audioCtx.decodeAudioData(ab);
    const out = audioCtx.createBufferSource();
    out.buffer = decoded;
    out.connect(audioCtx.destination);
    out.onended = () => {
      playing = false;
      if (playQueue.length) playWavArrayBuffer(playQueue.shift());
    };
    playing = true;
    out.start();
  } catch (e) {
    console.error("decode/play failed:", e);
  }
}

function enqueueOrPlay(ab) {
  if (playing) playQueue.push(ab);
  else playWavArrayBuffer(ab);
}

/* ------------ Message Handler ----------- */
async function handleServerMessage(ev) {
  try {
    /* Case 1: Browser gives Blob (true binary) */
    if (ev.data instanceof Blob) {
      const ab = await ev.data.arrayBuffer();
      enqueueOrPlay(ab);
      return;
    }

    /* Case 2: Browser gives ArrayBuffer directly (rare) */
    if (ev.data instanceof ArrayBuffer) {
      enqueueOrPlay(ev.data);
      return;
    }

    /* Case 3: Browser gives string (JSON or plain text) */
    if (typeof ev.data === "string") {
      try {
        const obj = JSON.parse(ev.data);
        console.log("Server JSON:", obj);

        // Show ASR/NLU text if provided
        if (obj.text || obj.asr_text) {
          setStatus("ASR: " + (obj.text || obj.asr_text), "blue");
        }

        // Base64 audio inside JSON (e.g., {audio_base64:"..."} )
        const audioKeys = ["audio_base64","audio","tts","tts_audio","wav_base64","data"];
        for (const key of audioKeys) {
          if (obj[key] && typeof obj[key] === "string") {
            const b64 = obj[key];
            try {
              const binary = atob(b64);
              const buf = new Uint8Array(binary.length);
              for (let i = 0; i < binary.length; i++)
                buf[i] = binary.charCodeAt(i);
              enqueueOrPlay(buf.buffer);
              return;
            } catch (err) {
              console.warn("Base64 decode failed for key:", key, err);
            }
          }
        }

        // No audio found → just a text event (e.g., tts_start/tts_end)
        return;
      } catch (jsonErr) {
        console.log("Server text:", ev.data); // plain text (non-JSON)
        return;
      }
    }

    console.warn("Unknown message type:", ev.data);
  } catch (err) {
    console.error("onmessage handler error:", err);
  }
}

/* ------------ Start / Stop Logic -------- */
startBtn.onclick = async () => {
  if (running) return;

  audioCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: SAMPLE_RATE });

  try {
    mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
  } catch (e) {
    setStatus("Microphone permission denied", "red");
    return;
  }

  ws = new WebSocket(NGROK_WS);
  ws.binaryType = "blob"; // ensures binary frames arrive as Blob when possible

  ws.onopen = () => {
    setStatus("Connected to server", "green");
    startBtn.disabled = true;
    stopBtn.disabled = false;
  };

  ws.onmessage = handleServerMessage;

  ws.onclose = () => {
    setStatus("Server connection closed", "red");
  };

  /* Microphone → PCM16 → WebSocket */
  micCtx = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: SAMPLE_RATE });
  sourceNode = micCtx.createMediaStreamSource(mediaStream);
  processor = micCtx.createScriptProcessor(1024, 1, 1);

  processor.onaudioprocess = (e) => {
    const input = e.inputBuffer.getChannelData(0);
    const buffer = new ArrayBuffer(input.length * 2);
    const view = new DataView(buffer);
    for (let i = 0; i < input.length; i++) {
      let s = Math.max(-1, Math.min(1, input[i]));
      view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
    }
    if (ws && ws.readyState === WebSocket.OPEN) ws.send(buffer);
  };

  sourceNode.connect(processor);
  processor.connect(micCtx.destination);

  running = true;
  setStatus("Mic active", "green");
};

stopBtn.onclick = () => {
  if (!running) return;
  running = false;

  try { processor.disconnect(); sourceNode.disconnect(); } catch {}
  try { mediaStream.getTracks().forEach(t => t.stop()); } catch {}
  try { ws.close(); } catch {}
  try { micCtx.close(); } catch {}
  startBtn.disabled = false;
  stopBtn.disabled = true;
  setStatus("Stopped", "black");
};
</script>
</body>
</html>
